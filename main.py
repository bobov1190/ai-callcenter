# main.py
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from mistralai import Mistral
from faster_whisper import WhisperModel
import torch
import tempfile
import os
import io
from pathlib import Path
from dotenv import load_dotenv
import numpy as np
from scipy.io import wavfile
import base64

load_dotenv()

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Faster-Whisper...")
whisper_model = WhisperModel("base", device="cpu", compute_type="int8")
print("‚úÖ Whisper –∑–∞–≥—Ä—É–∂–µ–Ω")

print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Silero TTS...")
device = torch.device('cpu')
torch.set_num_threads(4)

model, _ = torch.hub.load(
    repo_or_dir='/silero-models',
    model='silero_tts',
    language='ru',
    speaker='v3_1_ru',
    source='local'
)
model.to(device)
print("‚úÖ Silero TTS –∑–∞–≥—Ä—É–∂–µ–Ω")

mistral_client = Mistral(api_key=os.getenv("MISTRAL_API_KEY"))

class AICallCenter:
    def __init__(self):
        self.conversations = {}
    
    def transcribe_audio(self, audio_path: str) -> str:
        try:
            segments, info = whisper_model.transcribe(
                audio_path, 
                language="ru",
                vad_filter=True
            )
            text = " ".join([segment.text for segment in segments])
            return text.strip()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Whisper: {e}")
            return ""
    
    def get_ai_response(self, user_text: str, call_id: str = "default") -> str:
        if call_id not in self.conversations:
            self.conversations[call_id] = [
                {
                    "role": "system",
                    "content": """–¢—ã –≤–µ–∂–ª–∏–≤—ã–π –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä –∫–æ–ª–ª-—Ü–µ–Ω—Ç—Ä–∞. 
                    –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ–≥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞–º, –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.
                    –í—Å–µ–≥–¥–∞ –±—É–¥—å –≤–µ–∂–ª–∏–≤ –∏ –¥—Ä—É–∂–µ–ª—é–±–µ–Ω. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."""
                }
            ]
        
        self.conversations[call_id].append({
            "role": "user",
            "content": user_text
        })
        
        try:
            response = mistral_client.chat.complete(
                model="mistral-large-latest",
                messages=self.conversations[call_id],
                max_tokens=150
            )
            
            ai_text = response.choices[0].message.content
            
            self.conversations[call_id].append({
                "role": "assistant",
                "content": ai_text
            })
            
            return ai_text
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Mistral: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –≤–∞—à –≤–æ–ø—Ä–æ—Å."
    
    def synthesize_speech(self, text: str) -> bytes:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞—É–¥–∏–æ –∫–∞–∫ bytes –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞"""
        try:
            audio = model.apply_tts(
                text=text,
                speaker='xenia',
                sample_rate=24000
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ WAV bytes —á–µ—Ä–µ–∑ scipy
            buffer = io.BytesIO()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º tensor –≤ numpy array
            audio_np = audio.cpu().numpy()
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ int16
            audio_int16 = (audio_np * 32767).astype(np.int16)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ buffer
            wavfile.write(buffer, 24000, audio_int16)
            buffer.seek(0)
            
            return buffer.read()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ TTS: {e}")
            raise

call_center = AICallCenter()

@app.post("/api/voice-message")
async def voice_message(
    audio: UploadFile = File(...),
    call_id: str = "default"
):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_file:
            content = await audio.read()
            temp_file.write(content)
            temp_path = temp_file.name
        
        print(f"üìû –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è {call_id}")
        
        # 1. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
        print("üé§ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏...")
        user_text = call_center.transcribe_audio(temp_path)
        print(f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_text}")
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        os.remove(temp_path)
        
        if not user_text:
            user_text = "..."
            ai_response = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –≤–∞—Å –Ω–µ —Ä–∞—Å—Å–ª—ã—à–∞–ª. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞."
        else:
            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
            ai_response = call_center.get_ai_response(user_text, call_id)
            print(f"ü§ñ AI: {ai_response}")
        
        # 3. –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏
        print("üîä –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏...")
        audio_bytes = call_center.synthesize_speech(ai_response)
        
        print("‚úÖ –ì–æ—Ç–æ–≤–æ!")
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –≤ base64 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π)
        user_text_b64 = base64.b64encode(user_text.encode('utf-8')).decode('ascii')
        ai_response_b64 = base64.b64encode(ai_response.encode('utf-8')).decode('ascii')
        
        return StreamingResponse(
            io.BytesIO(audio_bytes),
            media_type="audio/wav",
            headers={
                "X-User-Text": user_text_b64,
                "X-AI-Response": ai_response_b64,
            }
        )
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/text-only")
async def text_only(data: dict):
    user_text = data.get("text", "")
    call_id = data.get("call_id", "test")
    
    if not user_text:
        raise HTTPException(status_code=400, detail="Text is required")
    
    ai_response = call_center.get_ai_response(user_text, call_id)
    
    return {
        "user_text": user_text,
        "ai_response": ai_response,
        "call_id": call_id
    }

@app.post("/api/end-call/{call_id}")
async def end_call(call_id: str):
    if call_id in call_center.conversations:
        del call_center.conversations[call_id]
    return {"status": "ok", "message": f"Call {call_id} ended"}

@app.get("/api/health")
async def health():
    return {
        "status": "ok",
        "whisper": "ready",
        "mistral": "ready",
        "silero": "ready"
    }

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É static –µ—Å–ª–∏ –Ω–µ—Ç
os.makedirs("static", exist_ok=True)

app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
async def root():
    return FileResponse("static/index.html")

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)